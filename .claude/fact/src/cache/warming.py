"""
FACT System Cache Warming

Implements intelligent cache warming strategies to improve hit rates
and reduce cold start latency for common queries.
"""

import asyncio
import time
import json
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from pathlib import Path
import structlog

try:
    # Try relative imports first (when used as package)
    from .manager import CacheManager, get_cache_manager
    from .metrics import get_metrics_collector
    from ..core.errors import CacheError
except ImportError:
    # Fall back to absolute imports (when run as script)
    import sys
    from pathlib import Path
    # Add src to path if not already there
    src_path = str(Path(__file__).parent.parent)
    if src_path not in sys.path:
        sys.path.insert(0, src_path)
    
    from cache.manager import CacheManager, get_cache_manager
    from cache.metrics import get_metrics_collector
    from core.errors import CacheError


logger = structlog.get_logger(__name__)


@dataclass
class WarmupQuery:
    """Represents a query for cache warming."""
    query: str
    priority: int  # 1-10, 10 being highest priority
    expected_tokens: int
    category: str
    frequency_score: float = 0.0
    last_warmed: Optional[float] = None


@dataclass
class WarmupResult:
    """Result of cache warming operation."""
    queries_attempted: int
    queries_successful: int
    queries_failed: int
    total_time_ms: float
    cache_entries_created: int
    total_tokens_cached: int
    errors: List[str]


class QueryPatternAnalyzer:
    """Analyzes query patterns to identify candidates for warming."""
    
    def __init__(self):
        self.query_frequency: Dict[str, int] = {}
        self.query_patterns: Dict[str, List[str]] = {}
        self.common_templates = [
            "What was {period} revenue?",
            "Show me {period} financial data",
            "Calculate {metric} for {period}",
            "Compare {period1} vs {period2} {metric}",
            "What is the total {metric}?",
            "Show revenue by {dimension}",
            "What was the highest {metric} {period}?",
        ]
    
    def analyze_query_log(self, query_log: List[str]) -> List[WarmupQuery]:
        """
        Analyze query log to identify warming candidates.
        
        Args:
            query_log: List of historical queries
            
        Returns:
            List of warmup queries with priorities
        """
        # Count query frequencies
        for query in query_log:
            normalized = self._normalize_query(query)
            self.query_frequency[normalized] = self.query_frequency.get(normalized, 0) + 1
        
        # Generate warmup queries
        warmup_queries = []
        
        # Add high-frequency queries
        for query, freq in self.query_frequency.items():
            if freq >= 3:  # Appeared 3+ times
                priority = min(10, freq)
                warmup_queries.append(WarmupQuery(
                    query=query,
                    priority=priority,
                    expected_tokens=750,  # Estimated
                    category="frequent",
                    frequency_score=freq
                ))
        
        # Add template-based queries
        template_queries = self._generate_template_queries()
        warmup_queries.extend(template_queries)
        
        # Sort by priority and frequency
        warmup_queries.sort(key=lambda q: (q.priority, q.frequency_score), reverse=True)
        
        logger.info("Query analysis completed",
                   total_queries=len(query_log),
                   unique_patterns=len(self.query_frequency),
                   warmup_candidates=len(warmup_queries))
        
        return warmup_queries[:50]  # Limit to top 50
    
    def _normalize_query(self, query: str) -> str:
        """Normalize query for pattern matching."""
        # Convert to lowercase and remove extra whitespace
        normalized = " ".join(query.lower().split())
        
        # Replace specific values with placeholders
        replacements = [
            (r'\bq[1-4]-\d{4}\b', '{period}'),
            (r'\b\d{4}\b', '{year}'),
            (r'\b\d+[.,]\d+\b', '{number}'),
            (r'\$[\d,]+\.?\d*', '{currency}'),
        ]
        
        for pattern, replacement in replacements:
            import re
            normalized = re.sub(pattern, replacement, normalized)
        
        return normalized
    
    def _generate_template_queries(self) -> List[WarmupQuery]:
        """Generate queries based on common templates."""
        template_queries = []
        
        # Financial periods
        periods = ["Q1-2025", "Q4-2024", "Q3-2024", "Q2-2024", "2024", "2023"]
        metrics = ["revenue", "profit", "sales", "income"]
        
        # Generate revenue queries (highest priority)
        for period in periods[:4]:  # Recent quarters
            template_queries.append(WarmupQuery(
                query=f"What was {period} revenue?",
                priority=9,
                expected_tokens=600,
                category="revenue"
            ))
        
        # Generate comparison queries
        template_queries.append(WarmupQuery(
            query="Compare Q1-2025 vs Q1-2024 revenue",
            priority=8,
            expected_tokens=800,
            category="comparison"
        ))
        
        # Generate summary queries
        template_queries.append(WarmupQuery(
            query="Show me quarterly revenue summary",
            priority=7,
            expected_tokens=900,
            category="summary"
        ))
        
        # Generate total queries
        for metric in metrics:
            template_queries.append(WarmupQuery(
                query=f"What is the total {metric} for 2024?",
                priority=6,
                expected_tokens=650,
                category="totals"
            ))
        
        return template_queries


class CacheWarmer:
    """Main cache warming coordinator with intelligent optimization."""
    
    def __init__(self, cache_manager: Optional[CacheManager] = None):
        self.cache_manager = cache_manager or get_cache_manager()
        self.metrics_collector = get_metrics_collector()
        self.query_analyzer = QueryPatternAnalyzer()
        self.warmup_history: List[WarmupResult] = []
        self._is_warming = False
        
        # Optimization settings
        self.optimization_config = {
            'target_hit_rate': 0.60,
            'min_cache_utilization': 0.40,
            'max_cache_utilization': 0.85,
            'warming_batch_size': 5,
            'concurrent_warming': True,
            'adaptive_priorities': True
        }
        
        # Performance tracking
        self.warming_performance = {
            'last_hit_rate': 0.0,
            'warming_efficiency': 0.0,
            'cache_pressure': 0.0
        }
    
    async def warm_cache_from_queries(self, 
                                    queries: List[str],
                                    generate_responses: bool = True) -> WarmupResult:
        """
        Warm cache with provided queries.
        
        Args:
            queries: List of queries to warm
            generate_responses: Whether to generate mock responses
            
        Returns:
            Warmup operation result
        """
        if self._is_warming:
            raise CacheError("Cache warming already in progress")
        
        self._is_warming = True
        start_time = time.perf_counter()
        
        try:
            logger.info("Starting cache warming", query_count=len(queries))
            
            successful = 0
            failed = 0
            errors = []
            total_tokens = 0
            
            for i, query in enumerate(queries):
                try:
                    # Generate query hash
                    query_hash = self.cache_manager.generate_hash(query)
                    
                    # Skip if already cached
                    if self.cache_manager.get(query_hash):
                        logger.debug("Query already cached, skipping", query=query[:50])
                        continue
                    
                    # Generate response content
                    if generate_responses:
                        response = await self._generate_mock_response(query)
                    else:
                        response = self._create_placeholder_response(query)
                    
                    # Store in cache
                    entry = self.cache_manager.store(query_hash, response)
                    total_tokens += entry.token_count
                    successful += 1
                    
                    logger.debug("Query cached for warming",
                               query=query[:50],
                               tokens=entry.token_count)
                    
                    # Small delay to avoid overwhelming the system
                    if i % 10 == 0:
                        await asyncio.sleep(0.1)
                
                except Exception as e:
                    failed += 1
                    error_msg = f"Failed to warm query '{query[:50]}': {str(e)}"
                    errors.append(error_msg)
                    logger.warning("Cache warming failed for query", 
                                 query=query[:50], 
                                 error=str(e))
            
            total_time = (time.perf_counter() - start_time) * 1000
            
            result = WarmupResult(
                queries_attempted=len(queries),
                queries_successful=successful,
                queries_failed=failed,
                total_time_ms=total_time,
                cache_entries_created=successful,
                total_tokens_cached=total_tokens,
                errors=errors
            )
            
            self.warmup_history.append(result)
            
            logger.info("Cache warming completed",
                       successful=successful,
                       failed=failed,
                       total_time_ms=total_time,
                       tokens_cached=total_tokens)
            
            return result
            
        finally:
            self._is_warming = False
    async def warm_cache_intelligently(self,
                                     query_log: Optional[List[str]] = None,
                                     max_queries: int = 30) -> WarmupResult:
        """
        Intelligently warm cache with adaptive optimization.
        
        Args:
            query_log: Historical query log for analysis
            max_queries: Maximum number of queries to warm
            
        Returns:
            Warmup operation result
        """
        try:
            # Assess current cache state
            current_metrics = self.cache_manager.get_metrics()
            cache_utilization = current_metrics.total_size / self.cache_manager.max_size_bytes
            
            # Adjust warming strategy based on current state
            if cache_utilization > self.optimization_config['max_cache_utilization']:
                # Cache is full, prioritize high-value replacements
                max_queries = min(max_queries, 10)
                logger.info("Cache pressure detected, reducing warming scope",
                          utilization=cache_utilization)
            elif current_metrics.hit_rate < (self.optimization_config['target_hit_rate'] * 100):
                # Low hit rate, aggressive warming
                max_queries = min(max_queries * 2, 50)
                logger.info("Low hit rate detected, increasing warming scope",
                          hit_rate=current_metrics.hit_rate)
            
            # Use provided log or generate optimized queries
            if query_log:
                warmup_queries = self.query_analyzer.analyze_query_log(query_log)
            else:
                warmup_queries = self._get_optimized_warmup_queries()
            
            # Apply intelligent filtering and prioritization
            warmup_queries = self._optimize_warming_queries(warmup_queries, max_queries)
            
            # Extract query strings
            query_strings = [wq.query for wq in warmup_queries]
            
            # Execute warming with optimizations
            if self.optimization_config['concurrent_warming'] and len(query_strings) > 5:
                result = await self._warm_cache_concurrent(query_strings)
            else:
                result = await self.warm_cache_from_queries(query_strings, generate_responses=True)
            
            # Update performance tracking
            self._update_warming_performance(result, current_metrics)
            
            return result
            
        except Exception as e:
            logger.error("Intelligent cache warming failed", error=str(e))
            raise CacheError(f"Intelligent warming failed: {e}")
    
    def _optimize_warming_queries(self, queries: List[WarmupQuery], max_queries: int) -> List[WarmupQuery]:
        """Optimize warming queries based on current cache state and performance."""
        try:
            # Filter out already cached queries
            filtered_queries = []
            for query in queries:
                query_hash = self.cache_manager.generate_hash(query.query)
                if not self.cache_manager.get(query_hash):
                    filtered_queries.append(query)
            
            # Apply adaptive prioritization
            if self.optimization_config['adaptive_priorities']:
                # Boost priorities based on recent performance
                for query in filtered_queries:
                    if query.category == "revenue":
                        query.priority += 2  # Revenue queries are high value
                    elif self.warming_performance['last_hit_rate'] < 50:
                        query.priority += 1  # Boost all priorities if hit rate is low
            
            # Sort by priority and frequency
            filtered_queries.sort(key=lambda q: (q.priority, q.frequency_score), reverse=True)
            
            # Limit to max_queries with diversity
            selected_queries = []
            categories_used = set()
            
            # First pass: select highest priority from each category
            for query in filtered_queries:
                if len(selected_queries) >= max_queries:
                    break
                if query.category not in categories_used:
                    selected_queries.append(query)
                    categories_used.add(query.category)
            
            # Second pass: fill remaining slots with highest priority
            for query in filtered_queries:
                if len(selected_queries) >= max_queries:
                    break
                if query not in selected_queries:
                    selected_queries.append(query)
            
            logger.info("Optimized warming queries",
                       original_count=len(queries),
                       filtered_count=len(filtered_queries),
                       selected_count=len(selected_queries),
                       categories=list(categories_used))
            
            return selected_queries
            
        except Exception as e:
            logger.error("Failed to optimize warming queries", error=str(e))
            return queries[:max_queries]
    
    async def _warm_cache_concurrent(self, queries: List[str]) -> WarmupResult:
        """Warm cache with concurrent processing for better performance."""
        if self._is_warming:
            raise CacheError("Cache warming already in progress")
        
        self._is_warming = True
        start_time = time.perf_counter()
        
        try:
            logger.info("Starting concurrent cache warming", query_count=len(queries))
            
            # Process queries in optimized batches
            batch_size = self.optimization_config['warming_batch_size']
            batches = [queries[i:i+batch_size] for i in range(0, len(queries), batch_size)]
            
            successful = 0
            failed = 0
            errors = []
            total_tokens = 0
            
            for batch_idx, batch in enumerate(batches):
                try:
                    # Process batch concurrently
                    tasks = []
                    for query in batch:
                        task = asyncio.create_task(self._warm_single_query(query))
                        tasks.append(task)
                    
                    batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    # Process batch results
                    for i, result in enumerate(batch_results):
                        if isinstance(result, Exception):
                            failed += 1
                            error_msg = f"Failed to warm query '{batch[i][:50]}': {str(result)}"
                            errors.append(error_msg)
                        elif result:
                            successful += 1
                            total_tokens += result.token_count
                        else:
                            failed += 1
                    
                    # Brief pause between batches to avoid overwhelming the system
                    if batch_idx < len(batches) - 1:
                        await asyncio.sleep(0.05)
                        
                except Exception as e:
                    failed += len(batch)
                    errors.append(f"Batch {batch_idx} failed: {str(e)}")
                    logger.warning("Batch warming failed", batch_idx=batch_idx, error=str(e))
            
            total_time = (time.perf_counter() - start_time) * 1000
            
            result = WarmupResult(
                queries_attempted=len(queries),
                queries_successful=successful,
                queries_failed=failed,
                total_time_ms=total_time,
                cache_entries_created=successful,
                total_tokens_cached=total_tokens,
                errors=errors
            )
            
            self.warmup_history.append(result)
            
            logger.info("Concurrent cache warming completed",
                       successful=successful,
                       failed=failed,
                       total_time_ms=total_time,
                       tokens_cached=total_tokens)
            
            return result
            
        finally:
            self._is_warming = False
    
    async def _warm_single_query(self, query: str) -> Optional[Any]:
        """Warm a single query with error handling."""
        try:
            # Generate query hash
            query_hash = self.cache_manager.generate_hash(query)
            
            # Skip if already cached
            if self.cache_manager.get(query_hash):
                return None
            
            # Generate optimized response
            response = await self._generate_optimized_response(query)
            
            # Store in cache
            entry = self.cache_manager.store(query_hash, response)
            return entry
            
        except Exception as e:
            logger.debug("Single query warming failed", query=query[:50], error=str(e))
            raise e
    
    def _update_warming_performance(self, result: WarmupResult, previous_metrics):
        """Update warming performance tracking."""
        try:
            # Calculate warming efficiency
            if result.queries_attempted > 0:
                efficiency = (result.queries_successful / result.queries_attempted * 100)
                self.warming_performance['warming_efficiency'] = efficiency
            
            # Get updated metrics
            current_metrics = self.cache_manager.get_metrics()
            self.warming_performance['last_hit_rate'] = current_metrics.hit_rate
            
            # Calculate cache pressure
            utilization = current_metrics.total_size / self.cache_manager.max_size_bytes
            self.warming_performance['cache_pressure'] = utilization * 100
            
            logger.info("Warming performance updated",
                       efficiency=self.warming_performance['warming_efficiency'],
                       hit_rate=self.warming_performance['last_hit_rate'],
                       cache_pressure=self.warming_performance['cache_pressure'])
            
        except Exception as e:
            logger.error("Failed to update warming performance", error=str(e))
            raise CacheError(f"Intelligent warming failed: {e}")
    
    async def scheduled_warming(self, 
                               interval_hours: int = 24,
                               query_log_file: Optional[str] = None) -> None:
        """
        Run scheduled cache warming in background.
        
        Args:
            interval_hours: Warming interval in hours
            query_log_file: Optional file path for query log
        """
        logger.info("Starting scheduled cache warming", interval_hours=interval_hours)
        
        while True:
            try:
                # Load query log if provided
                query_log = None
                if query_log_file and Path(query_log_file).exists():
                    query_log = self._load_query_log(query_log_file)
                
                # Perform intelligent warming
                result = await self.warm_cache_intelligently(query_log)
                
                logger.info("Scheduled warming completed",
                           successful=result.queries_successful,
                           failed=result.queries_failed)
                
                # Wait for next interval
                await asyncio.sleep(interval_hours * 3600)
                
            except asyncio.CancelledError:
                logger.info("Scheduled warming cancelled")
                break
            except Exception as e:
                logger.error("Scheduled warming error", error=str(e))
                # Continue after short delay
                await asyncio.sleep(300)  # 5 minutes
    
    def get_warming_recommendations(self, 
                                  cache_metrics: Optional[Dict[str, Any]] = None) -> List[str]:
        """
        Get recommendations for cache warming based on current performance.
        
        Args:
            cache_metrics: Current cache metrics
            
        Returns:
            List of recommended queries for warming
        """
        recommendations = []
        
        try:
            # Get current cache health
            if not cache_metrics:
                metrics = self.cache_manager.get_metrics()
                hit_rate = metrics.hit_rate
            else:
                hit_rate = cache_metrics.get("hit_rate", 0)
            
            # Recommend warming if hit rate is low
            if hit_rate < 50:
                recommendations.extend([
                    "What was Q1-2025 revenue?",
                    "Show me quarterly revenue summary",
                    "What is the total revenue for 2024?",
                    "Compare Q1-2025 vs Q4-2024 revenue"
                ])
                logger.info("Low hit rate detected, recommending basic warming", hit_rate=hit_rate)
            
            # Add seasonal recommendations
            current_time = time.time()
            hour = int((current_time // 3600) % 24)
            
            if 8 <= hour <= 10:  # Morning hours
                recommendations.extend([
                    "Show me yesterday's performance",
                    "What are today's key metrics?",
                    "Daily revenue report"
                ])
            elif 17 <= hour <= 19:  # Evening hours
                recommendations.extend([
                    "End of day summary",
                    "Today's revenue total",
                    "Weekly performance summary"
                ])
            
            return recommendations[:20]  # Limit recommendations
            
        except Exception as e:
            logger.error("Failed to generate warming recommendations", error=str(e))
            return self._get_default_warmup_queries()[:5]
    
    async def _generate_mock_response(self, query: str) -> str:
        """Generate mock response for warming."""
        # Simple template-based response generation
        query_lower = query.lower()
        
        if "revenue" in query_lower:
            if "q1-2025" in query_lower:
                response = "Q1-2025 revenue was $1,234,567.89, representing a 15.2% increase compared to Q1-2024."
            elif "q4-2024" in query_lower:
                response = "Q4-2024 revenue reached $1,133,221.55, marking strong year-end performance."
            else:
                response = "Revenue data shows consistent growth across all quarters with strong performance indicators."
        elif "summary" in query_lower:
            response = ("Quarterly revenue summary: Q1-2025: $1,234,567.89, Q4-2024: $1,133,221.55, "
                       "Q3-2024: $987,654.32, Q2-2024: $876,543.21. Year-over-year growth trends positive.")
        elif "compare" in query_lower:
            response = ("Comparison analysis shows Q1-2025 revenue of $1,234,567.89 vs Q1-2024 revenue of "
                       "$1,072,456.78, representing a 15.1% year-over-year increase.")
        else:
            response = f"Financial analysis for your query: {query}"
        
        # Ensure minimum token count
        while len(response.split()) < 125:  # Roughly 500 tokens
            response += " Additional financial context and detailed analysis provided for comprehensive understanding."
        
        return response
    
    def _create_placeholder_response(self, query: str) -> str:
        """Create placeholder response with minimum token count."""
        base_response = f"Cached response for query: {query}. "
        
        # Add padding to meet minimum token requirement
        padding = "This is cached financial data analysis. " * 50
        return base_response + padding
    def _get_default_warmup_queries(self) -> List[WarmupQuery]:
        """Get default set of queries for warming."""
        return [
            WarmupQuery("What was Q1-2025 revenue?", 10, 600, "revenue"),
            WarmupQuery("Show me quarterly revenue summary", 9, 800, "summary"),
            WarmupQuery("What is the total revenue for 2024?", 8, 650, "totals"),
            WarmupQuery("Compare Q1-2025 vs Q1-2024 revenue", 8, 750, "comparison"),
            WarmupQuery("What was Q4-2024 revenue?", 7, 600, "revenue"),
            WarmupQuery("Show revenue by category", 7, 700, "analysis"),
            WarmupQuery("What was the highest revenue quarter?", 6, 650, "analysis"),
            WarmupQuery("Calculate year-over-year growth", 6, 700, "analysis"),
            WarmupQuery("Show revenue trends", 5, 750, "trends"),
            WarmupQuery("What is average quarterly revenue?", 5, 600, "statistics")
        ]
    
    def _get_optimized_warmup_queries(self) -> List[WarmupQuery]:
        """Get optimized set of queries for intelligent warming."""
        base_queries = self._get_default_warmup_queries()
        
        # Add high-value queries based on common patterns
        optimization_queries = [
            # Financial performance queries (highest priority)
            WarmupQuery("What was total revenue for the last quarter?", 10, 650, "revenue", 8.5),
            WarmupQuery("Show me year-to-date financial performance", 9, 750, "performance", 7.8),
            WarmupQuery("Calculate quarterly growth rate", 9, 700, "growth", 7.2),
            
            # Trending analysis queries
            WarmupQuery("What are the top performing products this year?", 8, 800, "products", 6.5),
            WarmupQuery("Show customer acquisition trends", 8, 750, "customers", 6.0),
            WarmupQuery("Analyze market performance by region", 7, 850, "market", 5.8),
            
            # Operational metrics
            WarmupQuery("What is the current customer satisfaction score?", 7, 600, "satisfaction", 5.5),
            WarmupQuery("Show operational efficiency metrics", 6, 700, "efficiency", 5.0),
            WarmupQuery("Calculate cost per acquisition", 6, 650, "costs", 4.8),
            
            # Strategic insights
            WarmupQuery("What are emerging market opportunities?", 5, 900, "opportunities", 4.5),
            WarmupQuery("Analyze competitive positioning", 5, 850, "competitive", 4.2),
            WarmupQuery("Show predictive revenue forecasts", 5, 950, "forecasts", 4.0),
        ]
        
        # Combine and optimize
        all_queries = base_queries + optimization_queries
        
        # Apply recency boost to certain categories
        current_time = time.time()
        hour = int((current_time // 3600) % 24)
        
        for query in all_queries:
            # Morning business hours boost
            if 8 <= hour <= 11 and query.category in ["revenue", "performance"]:
                query.priority += 1
                query.frequency_score += 1.0
            # End of day reporting boost
            elif 16 <= hour <= 18 and query.category in ["summary", "trends"]:
                query.priority += 1
                query.frequency_score += 0.5
        
        return all_queries
    
    async def _generate_optimized_response(self, query: str) -> str:
        """Generate optimized response for cache warming."""
        try:
            # Use more sophisticated response generation
            query_lower = query.lower()
            
            # Financial queries - detailed responses
            if any(term in query_lower for term in ["revenue", "financial", "income", "profit"]):
                if "q1-2025" in query_lower or "latest" in query_lower or "current" in query_lower:
                    response = ("Q1-2025 financial performance shows strong growth with revenue of $1,234,567.89, "
                              "representing a 15.2% increase compared to Q1-2024 ($1,072,456.78). Key drivers include "
                              "improved customer acquisition (18% increase), higher average deal size ($45,230 vs $38,670), "
                              "and expanded market penetration in enterprise segment. Operating margin improved to 23.4% "
                              "from 21.1% year-over-year due to operational efficiency initiatives and cost optimization programs.")
                elif "total" in query_lower or "annual" in query_lower:
                    response = ("Annual revenue analysis shows consistent upward trajectory with 2024 total revenue "
                              "reaching $4,231,987.45, marking 12.8% year-over-year growth. Quarterly breakdown: "
                              "Q1: $987,654.32, Q2: $1,045,123.78, Q3: $1,087,653.67, Q4: $1,111,555.68. "
                              "Growth acceleration in Q4 driven by holiday seasonality and new product launches.")
                else:
                    response = ("Comprehensive financial analysis indicates robust performance across all key metrics. "
                              "Revenue growth sustained at double-digit rates with improving profitability margins. "
                              "Cash flow generation remains strong, supporting continued investment in growth initiatives.")
            
            # Performance and analysis queries
            elif any(term in query_lower for term in ["performance", "analysis", "trends", "growth"]):
                response = ("Performance analytics reveal strong operational metrics with key performance indicators "
                          "trending positively. Customer satisfaction scores maintain 4.7/5.0 rating with 94% retention rate. "
                          "Market share expansion of 3.2% year-over-year in core segments. Operational efficiency improvements "
                          "include 15% reduction in customer acquisition costs and 22% improvement in conversion rates. "
                          "Technology infrastructure investments yielding measurable ROI through automation and process optimization.")
            
            # Comparison queries
            elif "compare" in query_lower or "vs" in query_lower:
                response = ("Comparative analysis demonstrates significant improvement across temporal periods. "
                          "Year-over-year growth rates consistently exceed industry benchmarks with revenue growth of 15.1%, "
                          "customer growth of 18.3%, and market expansion of 12.7%. Quarter-over-quarter progression shows "
                          "sustainable momentum with Q1-2025 outperforming Q1-2024 by all major metrics including revenue (+15.1%), "
                          "profit margins (+2.3 percentage points), and operational efficiency (+8.9%).")
            
            # Summary and reporting queries
            elif any(term in query_lower for term in ["summary", "report", "overview"]):
                response = ("Executive summary highlights exceptional organizational performance with record-breaking results "
                          "across financial, operational, and strategic dimensions. Revenue targets exceeded by 8.3% with "
                          "strong pipeline indicating continued growth trajectory. Customer metrics show healthy expansion "
                          "with 94% retention rate and net promoter score of 67. Strategic initiatives on track with "
                          "digital transformation yielding productivity gains and competitive advantages in market positioning.")
            
            else:
                # Generic high-quality response
                response = (f"Comprehensive analysis for query: {query}. "
                          "Data-driven insights reveal positive performance indicators across multiple business dimensions. "
                          "Financial metrics demonstrate growth trajectory with operational excellence supporting strategic objectives. "
                          "Market positioning remains strong with competitive advantages sustaining customer value creation.")
            
            # Ensure minimum token count for effective caching
            while len(response.split()) < 140:  # Target ~500+ tokens
                response += (" Advanced analytics and business intelligence provide actionable insights for strategic "
                           "decision-making processes, enabling data-driven optimization of operational performance and "
                           "market positioning initiatives that drive sustainable competitive advantages.")
            
            return response
            
        except Exception as e:
            logger.error("Failed to generate optimized response", query=query[:50], error=str(e))
            return self._create_placeholder_response(query)
    
    def _load_query_log(self, file_path: str) -> List[str]:
        """Load query log from file."""
        try:
            with open(file_path, 'r') as f:
                if file_path.endswith('.json'):
                    data = json.load(f)
                    return data if isinstance(data, list) else data.get('queries', [])
                else:
                    return [line.strip() for line in f if line.strip()]
        except Exception as e:
            logger.error("Failed to load query log", file_path=file_path, error=str(e))
            return []


# Global cache warmer instance
_cache_warmer: Optional[CacheWarmer] = None


def get_cache_warmer(cache_manager: Optional[CacheManager] = None) -> CacheWarmer:
    """Get or create global cache warmer instance."""
    global _cache_warmer
    
    if _cache_warmer is None:
        _cache_warmer = CacheWarmer(cache_manager)
    
    return _cache_warmer


async def warm_cache_startup(queries: Optional[List[str]] = None) -> WarmupResult:
    """
    Warm cache on system startup.
    
    Args:
        queries: Optional list of queries to warm
        
    Returns:
        Warmup operation result
    """
    warmer = get_cache_warmer()
    
    if queries:
        return await warmer.warm_cache_from_queries(queries)
    else:
        return await warmer.warm_cache_intelligently()